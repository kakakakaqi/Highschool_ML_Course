{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0b81b2",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "_Welcome back! Today we’ll master Support Vector Machines — powerful, margin‑based classifiers that shine in high‑dimensional spaces._\n",
    "\n",
    "- **Linear SVM** — maximum‑margin hyperplanes\n",
    "- **Soft margins & hinge loss** — robust to overlap and noise\n",
    "- **Dual form & kernels** — non‑linear decision boundaries via the kernel trick\n",
    "- **Practical tuning** — scaling, `C`, `gamma`, and when to use `LinearSVC` vs `SVC`.\n",
    "\n",
    "---\n",
    "\n",
    "## What you’ll learn\n",
    "\n",
    "- The margin idea and why “wider is better.”\n",
    "- Hard-margin vs. soft-margin SVMs.\n",
    "- Primal, hinge-loss view; dual and kernels.\n",
    "- How to tune `C`, `gamma`, and choose a kernel.\n",
    "- Minimal scikit-learn code for linear and kernel SVMs.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) The SVM idea — separate with the **widest margin**\n",
    "\n",
    "Think of a line (or plane) that splits the classes. Among all lines that correctly split them, prefer the one that leaves the **largest gap** to the nearest points of either class. That gap is the **margin**. A larger margin usually means a simpler, more robust boundary that generalizes better.\n",
    "\n",
    "Given labeled points $(x_i, y_i)$ with $y_i \\in \\{-1,+1\\}$, SVM finds a hyperplane\n",
    "\n",
    "$$\n",
    "f(x) = w^\\top x + b\n",
    "$$\n",
    "\n",
    "that separates the classes while **maximizing the margin**\n",
    "\n",
    "- **Geometric margin** of a point: $\\displaystyle \\gamma_i = \\frac{y_i (w^\\top x_i + b)}{\\lVert w\\rVert}$\n",
    "- **Margin width** between class boundaries: $\\displaystyle \\frac{2}{\\lVert w\\rVert}$  \n",
    "  → Max margin $\\Longleftrightarrow$ minimize $\\lVert w\\rVert$ (subject to correct classification).\n",
    "\n",
    "There are two types of SVMs:\n",
    "\n",
    "- **Support Vector Classification** (SVC): in scikit-learn, $SVC$ is the kernel SVM classifier, while LinearSVC is the fast linear-only solver. It is for classification tasks\n",
    "- **Support Vector Regression** (SVR): for regression tasks\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Hard‑margin SVM (separable case)\n",
    "\n",
    "Here, every point must be on the correct side with **room to spare** (at least distance 1 in the scaled units). Minimizing $\\tfrac12\\lVert w\\rVert^2$ is equivalent to **maximizing the margin**. This version only works when data are perfectly separable; a single mislabeled or noisy point can break feasibility.\n",
    "\n",
    "**Optimization:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w,b}\\quad & \\tfrac12 \\lVert w\\rVert^2 \\\\\n",
    "\\text{s.t.}\\quad & y_i (w^\\top x_i + b) \\ge 1,\\quad i=1,\\dots,n\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Constraints enforce that every point sits **outside** the margin band.\n",
    "- Works only when data are perfectly separable.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Soft‑margin SVM (realistic case)\n",
    "\n",
    "Real data must overlap.\n",
    "\n",
    "We introduce **slack** variables $\\xi_i \\ge 0$ that measures how much a point breaks the margin rule:\n",
    "\n",
    "- $\\xi_i=0$ means safely outside\n",
    "- $0<\\xi_i<1$ means inside the margin but on the correct side\n",
    "- $\\xi_i>1$ means misclassified.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w,b,\\xi}\\quad & \\tfrac12 \\lVert w\\rVert^2 + C \\sum_{i=1}^n \\xi_i \\\\\n",
    "\\text{s.t.}\\quad & y_i (w^\\top x_i + b) \\ge 1 - \\xi_i,\\quad \\xi_i \\ge 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- The constant **C>0** balances two desires: keep the margin wide (small $\\lVert w\\rVert$), yet don’t allow too many/too large violations (small $\\sum\\xi_i$):\n",
    "  - large $C$ → penalize violations heavily (lower bias, higher variance, risking overfit)\n",
    "  - small $C$ → wider margin, more violations allowed (higher bias, lower variance, smoother, possibly underfit)\n",
    "\n",
    "**Hinge‑loss view (equivalent):**\n",
    "\n",
    "$$\n",
    "\\min_{w,b}\\quad \\frac{\\lambda}{2}\\lVert w\\rVert^2 + \\frac{1}{n}\\sum_{i=1}^n \\max\\!\\big(0, 1 - y_i(w^\\top x_i + b)\\big),\n",
    "$$\n",
    "\n",
    "with $\\lambda$ inversely related to $C$ (roughly, $\\lambda \\approx 1/(nC)$ in many libraries).\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Support vectors & the decision function\n",
    "\n",
    "Only points that lie **on or inside** the margin influence the final classifier; these are the **support vectors**. Points far from the boundary have zero hinge loss and do not change $w,b$. This is why SVM solutions are often sparse: the model depends on a subset of the training data.\n",
    "\n",
    "Points with zero loss **away from the margin** don’t affect the solution. The model depends only on a subset — the **support vectors** — that lie **on or inside** the margin band.\n",
    "\n",
    "Prediction:\n",
    "\n",
    "$$\n",
    "\\hat y = \\mathrm{sign}(w^\\top x + b).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Dual problem & the kernel trick (non‑linear SVM)\n",
    "\n",
    "The dual re-expresses the problem in terms of **similarities between pairs of points** via a kernel $K(x_i,x_j)$. Replacing dot-products with kernels lets the classifier act as if data were mapped into a higher-dimensional space **without computing that mapping explicitly** (the “kernel trick”). The prediction becomes a weighted sum over support vectors: only those with $\\alpha_i>0$ matter.\n",
    "\n",
    "The Lagrange dual of the soft‑margin problem (for kernel $K$) is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{\\alpha}\\quad & \\sum_{i=1}^n \\alpha_i - \\frac12 \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j\\, K(x_i,x_j) \\\\\n",
    "\\text{s.t.}\\quad & 0 \\le \\alpha_i \\le C,\\quad \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The decision function becomes:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^n \\alpha_i y_i\\, K(x_i, x) + b.\n",
    "$$\n",
    "\n",
    "**Common kernels $K(x,z)$**\n",
    "\n",
    "- **Linear:** $x^\\top z$ (useful for very high‑dimensional sparse features; scalable with `LinearSVC`).\n",
    "- **RBF (Gaussian):** $\\exp(-\\gamma \\lVert x - z\\rVert^2)$ with $\\gamma > 0$\n",
    "- **Polynomial:** $(\\gamma\\, x^\\top z + r)^d$ (degree $d$)\n",
    "- **Sigmoid:** $\\tanh(\\gamma\\, x^\\top z + r)$ (less common)\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "- `C` — regularization (as above)\n",
    "- `gamma` — RBF/poly scale; large `gamma` → tighter, more wiggly boundaries; small `gamma` → smoother\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Best practices\n",
    "\n",
    "- **Scale features** (standardize) — SVMs are distance‑based.\n",
    "- Start with **linear SVM** for many features/large $n$ (`LinearSVC` or `SGDClassifier(loss=\"hinge\")`).\n",
    "- Use **RBF SVC** for moderate $n$ when nonlinearity helps.\n",
    "- For **imbalanced** data, set `class_weight=\"balanced\"` or provide weights.\n",
    "- Enable probability estimates (Platt scaling) via `probability=True` in `SVC` (costs extra fitting).\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Minimal code — linear and RBF SVM (scikit‑learn)\n",
    "\n",
    "The flow below is: make data → train/test split → **pipeline** with `StandardScaler` → fit a **LinearSVC** (fast for large/high‑dimensional sets). Then try an **RBF SVC** and use a tiny **grid search** to pick `C` and `gamma`. `LinearSVC(dual=\"auto\")` chooses an efficient solver depending on the feature/sample ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c5c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Data\n",
    "X, y = make_classification(n_samples=2000, n_features=50, n_informative=10,\n",
    "                           class_sep=1.5, random_state=0)\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)\n",
    "\n",
    "# 1) Linear SVM (good for large & high-dim data)\n",
    "lin_clf = make_pipeline(StandardScaler(), LinearSVC(class_weight=\"balanced\"))\n",
    "lin_clf.fit(Xtr, ytr)\n",
    "print(\"LinearSVC acc:\", accuracy_score(yte, lin_clf.predict(Xte)))\n",
    "\n",
    "# 2) RBF-kernel SVM with small grid search (for moderate-sized data)\n",
    "rbf_pipe = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\"))\n",
    "param_grid = {\"svc__C\": [0.1, 1, 10], \"svc__gamma\": [\"scale\", 0.01, 0.1]}\n",
    "grid = GridSearchCV(rbf_pipe, param_grid, cv=3, n_jobs=-1)\n",
    "grid.fit(Xtr, ytr)\n",
    "print(\"Best RBF params:\", grid.best_params_)\n",
    "print(\"RBF acc:\", accuracy_score(yte, grid.predict(Xte)))\n",
    "print(classification_report(yte, grid.predict(Xte)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0aad12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8) (Optional) Hinge‑loss SGD — from scratch (toy)\n",
    "\n",
    "This toy optimizer does **stochastic subgradient descent** on the hinge loss plus $\\ell_2$ penalty. If a sample is correctly classified with margin $\\ge 1$, we only apply weight decay. If it violates the margin, we also step in the direction that reduces the hinge loss. This mirrors what large‑scale linear SVM libraries do under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652686aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sgd_linear_svm(X, y, lr=0.1, lam=1e-3, epochs=10):\n",
    "    # y in {-1, +1}\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d); b = 0.0\n",
    "    for _ in range(epochs):\n",
    "        idx = np.random.permutation(n)\n",
    "        for i in idx:\n",
    "            margin = y[i]*(X[i] @ w + b)\n",
    "            if margin < 1:\n",
    "                # subgradient of hinge + L2\n",
    "                w = (1 - lr*lam)*w + lr*y[i]*X[i]\n",
    "                b = b + lr*y[i]\n",
    "            else:\n",
    "                w = (1 - lr*lam)*w\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a3aaf5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9) How to choose `C`, `gamma`, and the kernel\n",
    "\n",
    "1. **Start simple:** linear vs RBF; pick by validation.\n",
    "2. **Grid search / log‑scale**: Use a small **log‑grid** `C ∈ {0.01, 0.1, 1, 10, 100}`, `gamma ∈ {\"scale\", 0.001, 0.01, 0.1, 1}`.\n",
    "3. **Watch for overfit:** very high training acc + lower validation acc → reduce `C` or `gamma`.\n",
    "4. **Speed:** If training is slow with kernel SVMs, reduce the grid size first and/or switch to `LinearSVC`.\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Multiclass strategies\n",
    "\n",
    "SVMs are inherently binary. For $K$ classes:\n",
    "\n",
    "- **One‑vs‑One (OvO):** train $K(K-1)/2$ binary classifiers (default in `SVC`). It compares every pair of classes and tends to work well when classes are balanced\n",
    "- **One‑vs‑Rest (OvR):** train $K$ classifiers vs the rest (default in `LinearSVC`). It is simpler and pairs well with linear models on high‑dimensional data.\n",
    "\n",
    "---\n",
    "\n",
    "## 11) FAQs & Gotchas\n",
    "\n",
    "Probability outputs from `SVC` come from an extra calibration step (Platt scaling); they can look conservative on small datasets. For speed problems, try fewer features, subsampling, or a linear model. Always re‑check that features are standardized.\n",
    "\n",
    "- **“My SVM is slow.”** → Too many samples with kernel SVC; try `LinearSVC` or sub-sample + RBF.\n",
    "- **“Predicted probabilities look odd.”** → They’re calibrated via Platt scaling; try `CalibratedClassifierCV`.\n",
    "- **“Decision boundary is jagged.”** → Likely large `gamma` (RBF) or very large `C`; reduce them and re‑scale features.\n",
    "- **“Imbalanced classes.”** → Use `class_weight=\"balanced\"` and evaluate with F1/ROC‑AUC, not just accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## 12) Quick cheat sheet\n",
    "\n",
    "Linear SVMs shine with many features (e.g., text). Kernel SVMs capture curvature at the cost of speed and tuning more hyperparameters. Both require scaling; both are sensitive to `C`, and kernels add `gamma` (and `degree` for polynomial).\n",
    "\n",
    "| Aspect              | Linear SVM                      | Kernel SVM (RBF/Poly)                 |\n",
    "| ------------------- | ------------------------------- | ------------------------------------- |\n",
    "| Nonlinearity        | No                              | Yes (via kernels)                     |\n",
    "| Scale with #samples | Great (use `LinearSVC`)         | Moderate/Slow for large $n$           |\n",
    "| Hyperparams         | `C`                             | `C`, `gamma`, (degree for poly)       |\n",
    "| Feature scaling     | **Required**                    | **Required**                          |\n",
    "| Typical use         | Text, high‑dim sparse, big data | Moderate data with nonlinear boundary |\n",
    "\n",
    "---\n",
    "\n",
    "## 13) Practice\n",
    "\n",
    "Try plotting the margin band (the two lines where $y(w^\\top x + b)=1$) on a 2‑D toy set to see which points become support vectors. Then vary `C` and watch how the number of support vectors and the margin width change.\n",
    "\n",
    "1. Standardize features, then compare `LinearSVC` vs `SVC(RBF)` on your dataset.\n",
    "2. Grid‑search `C` and `gamma`; report validation curves and best test score.\n",
    "3. Inspect support vectors: how many are there, and which points become SVs?\n",
    "4. Try class imbalance: set `class_weight=\"balanced\"` and compare metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- SVMs maximize margin → robust decision boundaries.\n",
    "- Soft margins + hinge loss handle overlap and noise.\n",
    "- Dual form enables **kernels** for powerful nonlinear separation.\n",
    "- In practice: **scale**, pick kernel by validation, tune `C`/`gamma`, and use `LinearSVC` for large datasets.\n",
    "\n",
    "**Next:** Principal component analysis + Dimensionality reduction"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
