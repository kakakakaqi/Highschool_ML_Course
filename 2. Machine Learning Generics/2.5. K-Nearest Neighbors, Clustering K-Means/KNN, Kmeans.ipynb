{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222d8505",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors & K-Means\n",
    "\n",
    "_Welcome back! Today we’ll build two core ML tools that you’ll use again and again._\n",
    "\n",
    "- **k-Nearest Neighbors (kNN)** — a simple, powerful **supervised** method\n",
    "- **K-Means** — a fast, practical **unsupervised** clustering method\n",
    "\n",
    "---\n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "- kNN intuition and how prediction works\n",
    "- Distances, feature scaling, and choosing **k**\n",
    "- K-Means objective and the Lloyd’s algorithm loop\n",
    "- How to pick **K** (elbow & silhouette)\n",
    "- Minimal, copy-pasteable code for both\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Vectors & basic distance (e.g., Euclidean)\n",
    "- Train/validation/test split\n",
    "- Python + NumPy/Matplotlib (optionally scikit-learn)\n",
    "\n",
    "---\n",
    "\n",
    "## 1) kNN — the core idea (intuition)\n",
    "\n",
    "It works on the idea that nearby points tend to share similar labels or values. The only real knob is **k**: small k focuses on very local patterns (can overfit to noise), while larger k smooths decisions (can miss fine detail). For classification, the class with the most support among the k neighbors is chosen; for regression, the neighbors’ values are averaged. Distance-weighting lets nearer neighbors count more than farther ones.\n",
    "\n",
    "> **“Tell me who your neighbors are, and I’ll tell you who you are.”**\n",
    "\n",
    "For a new point $x$:\n",
    "\n",
    "1. Find the **k closest** training points.\n",
    "2. **Classification:** majority vote of their labels.\n",
    "3. **Regression:** average their target values.\n",
    "\n",
    "```\n",
    "              ●  ●      Class A\n",
    "     x ?  →   ▲          Query\n",
    "              ○  ○      Class B\n",
    "```\n",
    "\n",
    "`x` takes the class of whichever neighbors are more common (or closer, if weighted).\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Distance & scaling (the make-or-break)\n",
    "\n",
    "Common distances for vectors $x, y$:\n",
    "\n",
    "- **Euclidean:** $\\lVert x - y\\rVert_2$ (default for geometry)\n",
    "- **Manhattan:** $\\lVert x - y\\rVert_1$ (robust to outliers)\n",
    "- **Cosine distance:** $1 - \\dfrac{x^\\top y}{\\lVert x\\rVert\\,\\lVert y\\rVert}$ — “angle” difference (great for text/high-dim sparse)\n",
    "\n",
    "**Euclidean** distance treats differences along each feature dimension equally, so features with larger scales can dominate if you don’t standardize. **Manhattan** distance is less sensitive to outliers and can work better with high-variance features. **Cosine** similarity ignores magnitude and compares only direction, which is useful for sparse/high‑dimensional data (e.g., text). Standardizing features (mean 0, std 1) before kNN keeps distances meaningful. If two classes are tied by count, distance‑weighted voting often resolves the tie sensibly.\n",
    "\n",
    "> ⚠️ So **Always scale features** (e.g., standardize to mean 0, std 1).  \n",
    "> Otherwise the feature with the largest units dominates distance.\n",
    "\n",
    "**Distance-weighted vote (optional):**  \n",
    "$w_i=\\dfrac{1}{\\operatorname{dist}(x,x_i)+\\epsilon}$ so closer neighbors count more.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Choosing **k** & common pitfalls\n",
    "\n",
    "Pick k by validation or cross‑validation: try several values and choose the one with the best validation score.\n",
    "\n",
    "- **Small k** (1–3): low bias, high variance → can be noisy.\n",
    "- **Larger k** (5–21+): smoother, higher bias → may underfit.\n",
    "- Pick **k** via validation or cross-validation.\n",
    "\n",
    "**Pitfalls**\n",
    "\n",
    "- Not scaling features.\n",
    "- Using an unfitting distance (e.g., Euclidean on sparse text).\n",
    "- Class imbalance: consider distance-weighted vote or class-weighted strategies.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) kNN — minimal code (easy to try)\n",
    "\n",
    "Before running the code below, here’s the flow you’ll see:\n",
    "\n",
    "- Make a small, separable dataset and split into train/test.\n",
    "- Fit the scaler **on training data only**, then transform both train and test.\n",
    "- Loop over a few k values; each model just stores the scaled training set.\n",
    "- Predict by finding the k nearest training points for each test point and aggregating their labels.\n",
    "\n",
    "> Tip: run locally. Install: `pip install numpy scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b2ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1) Toy 2D dataset\n",
    "X, y = make_classification(n_samples=400, n_features=2, n_redundant=0,\n",
    "                           n_informative=2, n_clusters_per_class=1, random_state=7)\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, random_state=7)\n",
    "\n",
    "# 2) Scale for distance-based methods\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr = scaler.transform(Xtr)\n",
    "Xte = scaler.transform(Xte)\n",
    "\n",
    "# 3) Try a few k values (distance-weighted)\n",
    "for k in [1, 3, 5, 11, 21]:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k, weights=\"distance\", metric=\"euclidean\")\n",
    "    clf.fit(Xtr, ytr)\n",
    "    acc = accuracy_score(yte, clf.predict(Xte))\n",
    "    print(f\"k={k:>2}  acc={acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cefdd1",
   "metadata": {},
   "source": [
    "**From-scratch (core idea):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7cb8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def knn_predict(Xtr, ytr, x, k=5):\n",
    "    d = ((Xtr - x)**2).sum(axis=1)**0.5          # Euclidean\n",
    "    idx = np.argpartition(d, k)[:k]              # top-k neighbors (unordered)\n",
    "    votes = np.bincount(ytr[idx])\n",
    "    return votes.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6164d341",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5) K-Means — the core idea (intuition)\n",
    "\n",
    "Choose **K** cluster centers (centroids) so that points are as close as possible to the centroid of the cluster they’re assigned to. The loss being minimized is the sum of **squared** distances to the nearest centroid, which naturally makes the mean the best representative for each cluster and favors compact, roughly spherical clusters.\n",
    "\n",
    "> **Goal:** group similar points into **K** clusters.  \n",
    "> Each cluster has a center (the **mean**), and points belong to their nearest center.\n",
    "\n",
    "**Plain-English objective:** place $K $ centers so points are as close as possible (on average, squared) to their assigned center.\n",
    "\n",
    "**Mathematically, with clusters $C_k$ and centroids $\\mu_k$:**\n",
    "\n",
    "$$\n",
    "\\min_{\\{C_k\\},\\{\\mu_k\\}} \\sum_{k=1}^{K} \\sum_{x\\in C_k} \\lVert x-\\mu_k\\rVert_2^2\n",
    "$$\n",
    "\n",
    "and the centroid update\n",
    "\n",
    "$$\n",
    "\\mu_k \\;=\\; \\frac{1}{\\lvert C_k\\rvert}\\sum_{x\\in C_k} x\\;.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Lloyd’s algorithm (the standard K-Means loop)\n",
    "\n",
    "Two repeated steps drive the method:\n",
    "\n",
    "1. **assignment** — give each point to its nearest centroid.\n",
    "2. **update** — move each centroid to the mean of its assigned points. Each step never increases the objective, so the loop converges to a local optimum. Initialization matters; using **k‑means++** spreads starting centroids apart and improves results. If a cluster becomes empty, re‑seed that centroid (e.g., pick a faraway point).\n",
    "\n",
    "Repeat until nothing changes.\n",
    "\n",
    "Each iteration never increases the objective → **converges** to a local optimum.\n",
    "\n",
    "**Good practice**\n",
    "\n",
    "- **k-means++** initialization (smart starting centers)\n",
    "- **Multiple restarts** (keep the best run)\n",
    "- **Scale features** before clustering\n",
    "- Handle empty clusters by re-seeding (e.g., to a farthest point)\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Picking **K** (how many clusters?)\n",
    "\n",
    "- **Elbow method:** plot $K$ vs **inertia** (sum of squared distances to centers).  \n",
    "  Look for the “bend” where adding clusters gives diminishing returns.\n",
    "- **Silhouette score** ($-1 \\ldots 1$): higher is better. Rough guide:  \n",
    "  $0.5+$ good separation, $\\sim 0.25$ weak structure.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) K-Means — minimal code (easy to try)\n",
    "\n",
    "Scan a few K values, compute inertia and silhouette for each, and then pick a reasonable K. Use `init=\"k-means++\"` and multiple `n_init` runs to avoid poor local optima. After selecting K, fit once more and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28510354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 1) Synthetic data with 4 clusters\n",
    "X, _, _ = make_blobs(n_samples=600, centers=4, cluster_std=1.2, random_state=42)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# 2) Try several K to compute inertia and silhouette\n",
    "Ks = range(2, 9)\n",
    "inertias, sils = [], []\n",
    "for K in Ks:\n",
    "    km = KMeans(n_clusters=K, n_init=10, init=\"k-means++\", random_state=42).fit(X)\n",
    "    inertias.append(km.inertia_)\n",
    "    sils.append(silhouette_score(X, km.labels_))\n",
    "\n",
    "print(\"K:\", list(Ks))\n",
    "print(\"Inertia:\", [round(v, 1) for v in inertias])\n",
    "print(\"Silhouette:\", [round(v, 3) for v in sils])\n",
    "\n",
    "# 3) Pick K (say best-looking), then fit once more\n",
    "bestK = 4\n",
    "km = KMeans(n_clusters=bestK, n_init=10, init=\"k-means++\", random_state=42).fit(X)\n",
    "print(\"Final silhouette at K=4:\", round(silhouette_score(X, km.labels_), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3088e7",
   "metadata": {},
   "source": [
    "**From-scratch (one iteration):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aa578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kmeans_step(X, centers):\n",
    "    # Assign\n",
    "    d = np.stack([np.linalg.norm(X - c, axis=1) for c in centers], axis=1)\n",
    "    labels = d.argmin(axis=1)\n",
    "    # Update\n",
    "    new_centers = np.array([X[labels == k].mean(axis=0) for k in range(centers.shape[0])])\n",
    "    return labels, new_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14451fe4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9) kNN vs K-Means — quick cheat sheet\n",
    "\n",
    "Use **kNN** when you have labels and want predictions; use **K‑Means** when you don’t have labels and want segments. kNN has almost no training cost but can be slow at prediction time; K‑Means costs more to fit but predicts fast by checking the nearest centroid. Scaling is important for both.\n",
    "\n",
    "| Aspect       | kNN (Supervised)                             | K-Means (Unsupervised)                      |\n",
    "| ------------ | -------------------------------------------- | ------------------------------------------- |\n",
    "| Purpose      | Predict labels/values from neighbors         | Discover groups without labels              |\n",
    "| “Training”   | Just store data                              | Learn K centers                             |\n",
    "| Compute cost | Cheap train, slower predict (need neighbors) | Fast predict, cost during fitting           |\n",
    "| Key choices  | **k**, distance, weighting, scaling          | **K**, init (k-means++), scaling, restarts  |\n",
    "| When to use  | Strong local patterns, simple baseline       | Quick segmentation, preprocessing, insights |\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Checklists, FAQs, and practice\n",
    "\n",
    "kNN checklist: scale features; tune **k** with validation; consider distance‑weighted voting; watch for class imbalance.\n",
    "\n",
    "K‑Means checklist: scale features; use k‑means++; try several K and compare inertia and silhouette; handle empty clusters robustly.\n",
    "\n",
    "Practice ideas: try kNN on Iris or a small MNIST subset with various k; for K‑Means, sweep K=2…10 and plot inertia and silhouette; as a speed‑up experiment, cluster first, then run kNN within each cluster and compare runtime/accuracy to vanilla kNN.\n",
    "\n",
    "**kNN checklist**\n",
    "\n",
    "- [ ] Scale features\n",
    "- [ ] Pick **k** via validation\n",
    "- [ ] Consider distance-weighted vote\n",
    "- [ ] Beware class imbalance\n",
    "\n",
    "**K-Means checklist**\n",
    "\n",
    "- [ ] Scale features\n",
    "- [ ] Use **k-means++** + multiple **n_init**\n",
    "- [ ] Pick **K** via elbow/silhouette\n",
    "- [ ] Watch for empty clusters (reseed)\n",
    "\n",
    "**FAQs**\n",
    "\n",
    "- _“kNN is slow at prediction.”_ → Use KD-Tree/Ball-Tree/ANN, or pre-cluster and search within cluster.\n",
    "- _“My clusters look weird.”_ → Scale features; try different K; check for non-spherical structure (K-Means likes spherical blobs).\n",
    "- _“Do I need labels for K-Means?”_ → No — it’s unsupervised.\n",
    "\n",
    "**Practice**\n",
    "\n",
    "1. On a real dataset (e.g., Iris or a MNIST subset), tune **k** for kNN and report accuracy.\n",
    "2. Run K-Means with $K = 2,\\dots,10$; plot inertia and silhouette; choose $K$ and visualize clusters.\n",
    "3. Hybrid: cluster first (K-Means), then run kNN **within each cluster** — compare speed/accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "kNN: predict using the answers from your **k nearest labeled neighbors**.\n",
    "\n",
    "K‑Means: place **K centroids** and alternate between assigning points to the nearest centroid and recomputing centroids until stable.\n",
    "\n",
    "You learned:\n",
    "\n",
    "- **kNN**: how neighbors + distances make predictions; how **k** and scaling affect results.\n",
    "- **K-Means**: how Lloyd’s algorithm works; how to pick **K** and get stable clusters.\n",
    "- Minimal code to try both methods today.\n",
    "\n",
    "**Next:** Support vector machines"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
